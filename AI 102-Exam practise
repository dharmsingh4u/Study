https://www.youtube.com/watch?v=FpyzoOzSwU0&t=512s

10:50 --> MP3 speech recogniser
12:53 --- >QnM maker service
15:21 --> automated caller
17:51 ---> bot service 
18:27 : 17 question

q 20, 21 22,22,28,29,30,32,36,37 -->38 continue, 55 questions done


https://www.examtopics.com/exams/microsoft/ai-102/view/2/
https://www.examtopics.com/exams/microsoft/ai-102/view/1/


***************************************************************************


Commiting your changes from your local to git hub:

You have to:

Init a local repository
Define the origin to the remote repository
Add the file to the index
Commit the files
Push the files from the local repository to the remote
It leads to something like that:

cd yourLocalFolder
git init
git remote add origin https://github.com/dharmsingh4u/Study.git
git add .
git commit -m "Initial commit"
git push -u origin master

force push to overwrite the files
git push -f origin master



*************************************************************************************************

Azure Open AI parameters :

temperature (range 0 to 2)
Use temperature to adjust randomness/creativity directly.

Lower temperature (0.1–0.3): More deterministic, focused responses.

Medium temperature (0.5–0.7): Balanced, creative but coherent.

Higher temperature (>0.9): More random and diverse outputs.

top_p
Use top_p to fine-tune the range of token diversity based on probability mass.

top_p = 1: No restriction — considers all tokens (pure sampling).

top_p = 0.9: Considers only the top tokens that make up 90% of the total probability.

top_p = 0.1: Very selective — only the most likely tokens are considered.

Combined usage: You can use both, but it's usually best to tune only one at a time (set the other to default).

{
  "model": "gpt-4",
  "prompt": "Write a poem about the ocean.",
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 100
}

This combination gives balanced creativity (temperature=0.7) while ensuring output comes from top 90% probable words (top_p=0.9).


*************************************************************************************************

Speech model :

upload the wav file if you have single file else upload a zip file containing multiple .wav files 
you can upload the mp4 file as as well
upload the transcript as well to validate the WER and TER rate for the model evluation 


Speech API end point 

https://<REGION_IDENTIFIER>.stt.speech.microsoft.com

stt-->speech to text 
tts-->text to speech


Custom Neural Voice model (text to speech model ):


Azure's Custom Neural Voice, a part of the Azure AI Speech service, allows you to create highly realistic, synthetic voices
 that mimic a specific person or voice actor.
 This can be used in various scenarios, including creating branded speech solutions, audiobooks, and more. 

How does it work :

1) Create a project in Speech Studio
2) Set up voice talent--->Before you can train a neural voice, you must submit a recording of 
		the voice talent's consent statement. The voice talent statement is a recording of the voice talent reading a statement that they consent to the usage of their speech data to train a custom voice model.
3) Prepare training data 
4) Train your voice model
5) Test
6) deploy



**************************************************************************************************
types of the promt engineering:


https://medium.com/@amiraryani/8-types-of-prompt-engineering-5322fff77bdf

Priming the Prompt:
a technique where the AI model is initially exposed to specific instructions or context before the primary prompt is given. This helps guide the AI towards a
 desired behavior or output by priming its understanding. 
 
 Imagine setting the stage for a play before the main act. Priming is like providing the AI with a script or a set of instructions to follow 
 before the main prompt is given.
 
 **********************************************************************************************************************
 
 
Multi-turn conversations:

Multi-turn conversations in an Azure chatbot allow for a more natural and engaging experience by enabling back-and-forth 
interactions between the user and the bot. This is achieved through features like follow-up prompts and active learning, 
allowing the chatbot to refine its responses based on user input and retain context across multiple turns


******************************************************************************************8

Azure conversational languege app:

1) Create the new applicaiton or project 
2) pick the model you want to create it i.e. text classfication or conversational model 
3) add intent
4) add uttrences to the intent. this can be added below 
	4.1) you can manually 
	4.2) upload json file 
	4.3) suggest uttrences by creating the new open AI service and deploying model under it

	
5) add entities 
6) train the model 
7) test the model 
9) deployt the model as an app service 


************************************************************************************************************


https://learn.microsoft.com/en-us/azure/ai-services/translator/text-translation/reference/v3/reference
Translator API:

To_language parameter is must have to pass in the request
from : not needed as it will use auto detect funaitonlity

https://api.cognitive.microsofttranslator.com/translate?api-version=3.0


https://learn.microsoft.com/en-us/azure/ai-services/translator/text-translation/reference/rest-api-guide?WT.mc_id=Portal-Microsoft_Azure_ProjectOxford

detector API :

gives us the language of the text

https://api.cognitive.microsofttranslator.com/detect?api-version=3.0


transliterate:

https://api.cognitive.microsofttranslator.com/transliterate?api-version=3.0


Below parameters :

api-version	Required parameter.
Version of the API requested by the client. Value must be 3.0.

language	Required parameter.
Specifies the language of the text to convert from one script to another. Possible languages are listed in the transliteration 
scope obtained by querying the service for its supported languages.

fromScript	Required parameter.
Specifies the script used by the input text. Look up supported languages using the transliteration scope, to find input scripts available for the selected language.

toScript	Required parameter.
Specifies the output script. Look up supported languages using the transliteration scope, to find output scripts available for the selected combination of input language and input script.
**************************************************************************************************************************************

AI translator :


custom glossary file (this file is always placed in target storage account):

You can use a custom glossary file to ensure that the specific terminology is consistently and accurately translated.

For example, if your organization has a trademarked name that you don't want translated, you can add that to the glossary file. 
You can also specify the capitalization to use for words, choose specific translations for ambiguous words, or designate 
specific meanings for your unique context.

for ex :

Source		Target
Bank       Banque
   Card     Carte
   Crane    Grue
   Office   Office
   Tiger    Tiger
   US       United States

Document translation:

Document translation is a cloud-based machine translation feature of the Azure AI Translator service. 
You can translate multiple and complex documents across all supported languages and dialects while preserving original document structure and data format

code

sourceUrl = "<your-source container-url>"
targetUrl = "<your-target-container-url>"
targetLanguage = "<target-language-code>"

poller = client.begin_translation(sourceUrl, targetUrl, targetLanguage)
result = poller.result()

you need to specfiy the custom glossary file as an extra optional parameter while doing document translation


************************************************************************************************


Anomaly Detector


Anomaly Detector is an Azure AI service that enables developers to monitor and detect anomalies in their time series data with little machine learning knowledge. 
The service provides a set of APIs that can be used for either batch validation or real-time inference. 
The underlying AI models are trained and customized using the user’s data, which allows the service to understand the unique needs of their business.

Anomaly Detector can detect spikes, dips, deviations from cyclic patterns, and trend changes through both univariate and multivariate APIs

The Anomaly Detector API provides two methods of anomaly detection: batch and streaming.


Univariate :

Analyzes each variable independently, looking for deviations from its historical patterns. 


Multivariate 

Considers the relationships between multiple variables to detect anomalies. 

*************************************************************************************************************************************


speech synthesizer

A speech synthesizer, also known as a text-to-speech (TTS) system, is a technology that converts written text into audible speech. 
It's used in various applications, including accessibility tools for screen readers, assistive technologies for
 individuals with speech impairments, and creating AI assistants with natural-sounding voices
 
Language identification (LID) use cases include:

Speech to text recognition when you need to identify the language in an audio source and then transcribe it to text.
Speech translation when you need to identify the language in an audio source and then translate it to another language


Azure speech to text :

has auto detect feature of language of incoming voice plus the translation to multiple or single langauges
 

**************************************************************************************************************

Azure search API 

index:

An index is the searchable content, similar to a database table. data structure that holds the searchable content, defined by a schema
Each document in the index corresponds to a row, and each field corresponds to a column, similar to a database table. 
It contains the data that the search engine uses to find relevant results. 

indexer :

the process that ingests and updates data from sources into the index.A process that extracts text from PDF files, performs OCR, and indexes it
various function it does :Data extraction, transformation, and loading from data sources

parallel indexing

using multiple indexers to process and load data into a search index concurrently, 
accelerating the process and improving performance for large datasets

************************************************************************************************************************************

QnA maker:

It an service that uses frequently questions answer as your knowledge bases and uses the azure ai search to fetch the required data from 
your PDF 

you can add chit-chat  feature as well along with your knowledge base which  makes it more conversational and engaging.
This can be a starting point for your bot's personality, and it will save you the time and cost of writing them from scratch .below are 


chit -chat file name can be like qna_chitchat_professional or qna_chitchit_friindly etc 
Personality	Example

Professional			Age doesn't really apply to me.
Friendly				I don't really have an age.
Witty					I'm age-free.
Caring					I don't have an age.
Enthusiastic			I'm a bot, so I don't have an age.


**********************************************************************************************************************


F1 Score


https://www.google.com/search?q=f1+score+meaning+in+machine+learning+simple&sca_esv=9a8ab6189a2ca03b&rlz=1C1GCEU_en&sxsrf=AHTn8zoAmLvcidgleOovAXo_rEDTbjPQ2g%3A1747665997589&ei=TUQraPnhI_SH4-EPkdOxuQo&ved=0ahUKEwi5vOOX46-NAxX0wzgGHZFpLKcQ4dUDCBA&uact=5&oq=f1+score+meaning+in+machine+learning+simple&gs_lp=Egxnd3Mtd2l6LXNlcnAiK2YxIHNjb3JlIG1lYW5pbmcgaW4gbWFjaGluZSBsZWFybmluZyBzaW1wbGUyBRAhGKABMgUQIRigAUiyXlAAWPxbcAB4AZABAJgBa6AB2RCqAQQyNi4yuAEDyAEA-AEBmAIcoAK0EsICCxAAGIAEGJECGIoFwgIREC4YgAQYsQMY0QMYgwEYxwHCAgsQLhiABBixAxiDAcICDhAuGIAEGLEDGIMBGIoFwgIREC4YgAQYsQMYgwEY1AIYigXCAg4QABiABBixAxiDARiKBcICCxAAGIAEGLEDGIMBwgIFEAAYgATCAgQQIxgnwgIKECMYgAQYJxiKBcICDRAAGIAEGLEDGEMYigXCAgoQABiABBhDGIoFwgIQEC4YgAQY0QMYQxjHARiKBcICDhAuGIAEGLEDGIMBGNQCwgIIEC4YgAQYsQPCAhAQABiABBixAxhDGIMBGIoFwgILEC4YgAQYxwEYrwHCAgUQLhiABMICBxAAGIAEGArCAgoQABiABBgUGIcCwgIKEAAYgAQYsQMYCsICBhAAGAcYHsICCxAAGIAEGIYDGIoFwgIGEAAYFhgewgIIEAAYgAQYogTCAgUQIRifBcICBxAhGKABGAqYAwDiAwUSATEgQJIHBDI0LjSgB7C6AbIHBDI0LjS4B7QSwgcMMC4yLjIyLjMuMC4xyAfIAQ&sclient=gws-wiz-serp



******************************************************************************************************************************


describeImageInStreamAsync in computer vision API


The describe feature of the Computer Vision API generates a human-readable sentence to describe the contents of an image. 
This is particularly useful for accessibility purposes, as it allows visually impaired users to understand what is in an image without needing to see it. 
The describe feature can also be customized to provide additional details or context, if desired.

readInStreamAsync is used for extracting text from images (OCR).
analyzeImagesByDomainInStreamAsync is for domain-specific image analysis (like celebrities or landmarks).
tagImageInStreamAsync provides tags for the image but not in complete sentences, which is less useful for accessibility purposes.


************************************************************************************************************************


LoggingOptOut in azure language/computer vision (question came )

A new query parameter, LoggingOptOut , is now available for customers who wish to opt out of logging
input text for incident reports.

By default, this parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment Analysis and Named Entity Recognition endpoints.

 The LoggingOptOut parameter is true by default for the PII and health feature endpoints. More information on the LoggingOptOut query parameter is available 
 in the API reference.
 
 
 Data sent in synchronous or asynchronous calls may be temporarily stored by Azure AI Language for up to 48 hours only 
 and is purged thereafter. This data is encrypted and is only accessible to authorized on call engineers when service
 support is needed for debugging purposes in the event of a catastrophic failure. To prevent this temporary storage of input data,
 the LoggingOptOut query parameter can be set accordingly
 
 
*******************************************************************************************************************************
Named Entity Recognition (NER) in computer vision API

Named Entity Recognition (NER) is a feature that identifies and classifies entities in text . ex : predifined entities like place, person ,
date, URL,Address etc

Custom NER allows you to train models to recognize specialized entities specific to your use case. In this you need to build custom entity model 
label your data --> train it --> test it---> deploy as an app service plan 


***********************************************************************************************************************************

channels in bot service azure
 
 
Direct Line: 	Enables you to integrate your bot into web pages, mobile apps, and other applications. 
Email: 	Allows your bot to communicate with users via Microsoft 365 email. 
Facebook: 	Supports communication with users on both Facebook Messenger and Facebook Workplace. 
GroupMe: 	Allows your bot to interact with users through the GroupMe platform. 
Other Channels: 	Azure Bot Service also supports other channels like Skype, Slack, Telegram, and more, allowing for a diverse range of user interactions.


Direct Line
Pmrimarily used for text-based conversations.

Direct Line Speech
specifically designed for voice-based interactions.
It provides text-to-speech and speech-to-text services within the channel. 
It allows a client to stream audio directly to the channel which will then be converted to text and sent to the bot. 
Direct Line Speech can also convert text messages from the bot into audio messages spoken by an AI-powered voice. 
Combined, this makes Direct Line Speech capable of having audio only conversations with clients.
Direct Line Speech is a robust, end-to-end solution for creating a flexible, extensible voice assistant. It is powered by the Bot Framework and its
Direct Line Speech channel, that is optimized for voice-in, voice-out interaction with bots

********************************************************************************************************************** 


Azure Cognitive Services RBAC roles (question regarding role  ) :
Cognitive Services User:
This role provides a wide range of access to Cognitive Services resources, including the ability to view, create, edit, and delete data, models, 
and endpoints in custom projects. It also allows users to list resource keys. 

QnA Maker Editor:
This role grants full access to QnA Maker resources, allowing users to create, edit, and publish knowledge bases. 
They can manage all aspects of QnA Maker, including adding, updating, and deleting Q&A pairs, and testing the knowledge base. 

QnA Maker Reader:
This role provides read-only access to QnA Maker resources, allowing users to view knowledge bases and test them. 
They cannot make any changes or modifications to the knowledge base


In essence:
If you need broad access to various Cognitive Services resources, use the "Cognitive Services User" role. 
If you need to manage and publish QnA Maker knowledge bases, use the "QnA Maker Editor" role. 
If you only need to view and test QnA Maker knowledge bases, use the "QnA Maker Reader" role. 


*************************************************************************************************
AI enrichment in Azure AI Search

https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-intro


Indexer process :

import --> document cracking --->field mapping -->enrichment(custom) ---> output field mapping (defineing projections) --> load the index 

field mappings 

field mappings that establish the data path between source fields in a supported data source and target fields in a search index.

*********************************************************************************************************


Projections :


tableName or containter name
Determines the name of a new table created in Azure Table Storage.

generatedKeyName	
Column name for the key that uniquely identifies each row. 
The value is system-generated. If you omit this property, a column will be created automatically that uses the table name and "key" as the naming convention.

source	
A path to a node in an enrichment tree. The node should be a reference to a complex shape that determines which columns are created in the table.


************************************************************************************************************************8

Azure AI text analytics in langauges services :

detect 
key pharese extraction
named entity resolution
sentiment analysis 
detection and translation 
Personal and health data information detection.
summarization


***************************************************************************************************************************
Azure Anomaly Detector vs Azure Metrics Advisor

Azure Anomaly Detector 
is a core AI service that detects anomalies in time series data using machine learning . Anomaly Detector is a more basic, code-first service, 

Azure Metrics Advisor 
builds upon it with advanced features like multi-dimensional analysis, root cause diagnosis, and real-time alerts.
offers a higher-level, more user-friendly experience with built-in features. 


*********************************************************************************************************

Azure speech authenfication without api key  (question on azure speech)

To authenticate with a Microsoft Entra token, the Speech resource must have a custom subdomain and use a private endpoint
he Speech service uses custom subdomains with private endpoints only.

custom subdomain:

Provides a custom domain name for your Azure resource, allowing you to use your own domain to access your service.
If you have a custom domain "mybusiness.com", you could create a custom subdomain like "store.mybusiness.com" for your Azure App Service. 

*********************************************************************************************************	

Azure open AI token /RPM (capcaity question )


capacity integer This represents the amount of quota you are assigning to this deployment

Azure OpenAI allows you to manage how frequently your application can make inferencing requests. 
Your rate limits are based on Tokens-per-Minute (TPM). For example, if you have a capacity of 1, 
this equals 1,000 TPM, and the rate limit of requests you can make per minute (RPM) is calculated using a ratio. For every 1,000 TPM, you can make 6 RPM.

6 RPM=1000 TPM     
1 Capacity = 1000 TPM


curl -X PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/resource-group-temp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-001/deployments/gpt-4o-test-deployment?api-version=2023-05-01 \
  -H "Content-Type: application/json" \
  -H 'Authorization: Bearer YOUR_AUTH_TOKEN' \
  -d '{"sku":{"name":"Standard","capacity":10},"properties": {"model": {"format": "OpenAI","name": "gpt-4o","version": "2024-11-20"}}}'

********************************************************************************************************************

Bot Framework Emulator

The Bot Framework Emulator is a desktop application that permits bot developers to test and debug their bots locally or remotely.
allows bot developers to test and debug bots built using the Bot Framework SDK



********************************************************************************************************


Service Endpoint (in Virtual networks)


for a virtual network provides a secure and direct connection to Azure services over the Azure backbone network, bypassing the public internet. 
This means resources within your virtual network can access Azure services using private IP addresses, rather than public IPs. 

if you restricy only specific resources can access azure ai services then do the following 

add service end point in the specfied the Vnet 1
Modify the the specfic azure service settign for the networks section




**********************************************************************************************88
Private end point :


*********************************************************************************************************8

Computer vision detect type (image type comes in the exam ) for clip art, line drawling , photograph

Categories—Categorizes image content according to the category taxonomy.
◦Description of caption—Describes the image content in a complete English sentence.
◦Color—Determines whether the image is black and white or color, and for colored images, detects the dominant and accent colors.
◦Tags—Tags the image with a detailed list of words related to the image content.
◦Faces—Detects if faces are present in the image.
◦Image Type—Detects if an image is a clip art or a line drawing.
◦Adult—Detects if the image is pornographic in nature or if the image depicts sexually suggestive content.
◦Objects—Detects various objects in the image.



***********************************************************************************************************************

Face API (live or fake photo )

Liveness detection is a security measure that ensures a biometric sample (like a face or fingerprint) is from a real, 
live person and not a fake image, video, or mask

Validating that subjects are real people can be done by detecting natural head movements. 
The HeadPose attribute provides the orientation of the head, and checking for natural changes in this attribute
 over time can help determine if the subject is a real person or a static image

*******************************************************************************************************************88

DOcument intelligence API (question came on file type i.e. raw image binary)

if you dont want to store the images in the cloud while calling the API then use the image file format as  raw image binary



**********************************************************************************************************************8


Knowledge base for the Q&A maker 


create_op = client.knowledgebase.create(create_kb_payload=create_kb_dto)

A knowledge base stores question and answer pairs for the CreateKbDTO object from these three sources:

For editorial content, use the QnADTO object
For files, use the FileDTO object. The FileDTO includes the filename and the public URL to reach the file.

For URLs, use a list of strings to represent publicly available URLs.


**************************************************************************************************

Entity in Conversational model :

Entities represent pieces of information extracted from user utterances that are relevant to understanding the user's intent. 
These entities can be classified into several types, 
including prebuilt, learned, list, and regular expression entities.


Entities type:

list:
These entities represent a fixed, closed set of related words or phrases, with synonyms. 
Aiport entity includes list of aiports id 

Prebuilt Entities:
These are pre-defined entities provided by the language model platform, such as dates, times, locations, and numbers. 

Machine Learning entities:
These entities are trained by the model to identify specific values based on examples provided during training. 

Regex Entities:
These entities are defined using regular expressions to capture specific patterns in the user's input. 

Geography V2":
encompasses various geographical locations. These include cities, countries, continents, and states, as defined by the Microsoft LUIS service. 
For example, "Paris", "France", "Asia", and "California" would all be considered instances of this entity type. 

****************************************************************************************************************




You are creating an application that references the Azure OpenAI REST API for a DALL-E model.
You plan to use thumbnails of the images that DALL-E generates and display them in a table on a webpage.
You need to find the image URLs in the JSON response.

Which element should you review?

Answer is-->result element 




You have an Azure App Services web app named App1.
You need to configure App1 to use Azure AI Services to authenticate by using Microsoft Entra ID. The solution must meet the following requirements:
Minimize administrative effort.
Use the principle of least privilege.
What should you do?

answer is---> From App1, enable a managed identity and assign role-based access control (RBAC) permissions to Azure AI Services.


ou are building an app that uses Azure AI Services Document Translation.
You need to improve the quality of the translation for user-uploaded documents.
What should you ask the users to include when they upload a document?

Answer is -->the source language


You are creating an orchestration workflow for Language Understanding.
You need to configure workflows for multiple languages. The solution must minimize administrative effort.
What should you create for each language?

answer is -->separate workflow projects 
orchestration workflow doesnt support multi languages model 


You are developing a video processing application that will utilize Azure AI Video Indexer to extract insights from videos containing content in multiple languages.

To configure the API calls for identifying and processing the multilingual content, you need to specify the correct parameter for language identification.

Which value should you set for the sourceLanguage parameter to ensure accurate multilingual detection?

answer is -->multi-language detection

****************************************************************************************************


Adding faces from file :



https://learn.microsoft.com/en-us/rest/api/face/face-recognition-operations/find-similar?view=rest-face-v1.2&tabs=HTTP	

************************************************************************************************************************


query keys vs admin keys in azure ai search 


admin keys grant read-write access, allowing creation, modification, and deletion of objects, while query keys provide
 read-only access for client applications to issue queries. Using query keys for client applications is recommended for security reasons. 

if query is lost or in danger then use the following:

1) add a new query key 
2) change the app to new query key 
3) delete the old key


*************************************************************************************************************


document-level filtering in azure ai search

Azure AI Search doesn't natively support row-level or document-level permissions. However, 
you can achieve document-level filtering by using a security filter pattern


1) identify the group membership of the user 
2) add this group to the index 
3) set the group as filterable and passes in the Search request.

POST https://[service name].search.windows.net/indexes/securedfiles/docs/search?api-version=2024-07-01

{
   "filter":"group_ids/any(g:search.in(g, 'group_id1, group_id2'))"  
}


************************************************************************************************************


prebuilt-read vs prebuilt-document (OR prebuilt-layout) in Azure document intelligence


prebuilt-read model 
focuses on extracting text from documents and  is primarily for extracting text from PDF , images etc . Basic OCR 

prebuilt-document model offers more comprehensive document analysis, including key-value pair extraction and table analysis.
Provides a more structured JSON output containing extracted data, including key-value pairs and table structures.


*************************************************************************************************************


https://learn.microsoft.com/en-us/azure/bot-service/bot-builder-basics?view=azure-bot-service-4.0

Entities ve properties differcen in azure bot service 

Tyepe of cards in 

https://learn.microsoft.com/en-us/azure/bot-service/bot-service-design-user-experience?view=azure-bot-service-4.0#cards
*****************************************************************************************************************
deploying bot service applications 

az webapp deployment source config-zip --resource-group "<resource-group-name>" --name "<name-of-app-service>" --src "<project-zip-path>"


Bot Framework Composer:
s a visual authoring canvas for building bots.Allows for visual editing of conversation flows, in-context editing for language understanding (NLU), 
and management of QnA components. 

Bot Framework Emulator

is a tool for testing and debugging bots.Enables interaction with the bot, observation of conversational flow, and verification of bot logic. 


types of dialogues

https://learn.microsoft.com/en-us/azure/bot-service/bot-builder-concept-dialog?view=azure-bot-service-4.0

waterflow 
adaptive 
prompt
compononet 
action 
etc 


************************************************************************************************************

Azure bot :

Dailogue trigger :

Dialog triggers handle dialog specific events that are related to the lifecycle of the dialog. 
There are currently six dialog triggers in the Bot Framework SDK

OnBeginDialog
OnCancelDialog
OnEndOfActions
OnError
OnRepromptDialog
OnDialog

activity handler (question came on custom text messages)

An activity handler is designed to handle various types of activities in the Microsoft Bot Framework, including events. 
You can use an activity handler to respond to events and send custom text responses based on the type of activity received.


send_activity :

is a method used to send messages and other types of activities from your bot to the user's channel. 
It's part of the TurnContext object, which represents the current interaction between the bot and the user.
 You use send_activity in your bot's activity handlers to respond to user input or trigger other actions. 
 

clear_state 

is used to remove or reset the bot's stored conversation state. This state typically includes information
 about the user's interaction history, any data collected during the conversation, and the bot's current dialog state
 
 
trace activity

 is an activity that your bot can send to the Bot Framework Emulator. You can use trace activities to interactively debug a bot,
 as they allow you to view information about your bot while it runs locally.

*************************************************
How to debug the bot using bot emulator :
*************************************************


Run a bot locally
Connect to a bot emulator running on localhost
Use trace activities for debugging


**************************************************************************************************************8

Open AI tokens
:

Prompt Tokens

These are the tokens in your instructions or input text that you provide to the model. They serve as the basis for the model's generation. 


Completion Tokens:
These are the tokens that the model generates as a response to your prompt. They are the output of the model based on the input. 


Max Tokens

This parameter sets the upper limit on the number of tokens the model can generate in its completion. 
It's crucial for managing costs and preventing excessive output, especially with models like GPT-4 
Turbo that have different limits on input and output. 

billing is done on the total number of tokens i.e. completed token + prompt token



System Prompt:
as the foundational instructions that dictate an AI's behavior. They establish the framework for how the AI will interact 
and respond, similar to a job description for an employee. These prompts define the AI's role, its area of expertise


User Prompt:

re the specific instructions or questions a user provides to an AI system to elicit a desired response . for ex 
These prompts ask the AI to categorize input data based on predefined labels or categories. For example,
a user might ask the AI to classify a product review as positive or negative.


Assitant:

Provides the actual response from the LLM based on the user's input and the system prompt. 
Example:
If the user asks "What is the capital of France?", the assistant prompt would provide the answer "The capital of France is Paris.

messages = [
    {'role': 'system', 'content': 'You complete the next turn in conversations using the following format: <name>: <message>'},
    {'role': 'user', 'content': 'Alice: What is your name?\nBob:Bob\nAlice: How are you today?'},
]

***********************************************************************************************************

Open AI create service.


az cognitiveservices account create -n myresource -g myResourceGroup --assign-identity --kind OpenAPI --sku S -l WestEurope --yes
    --encryption '{
      "keySource": "Microsoft.KeyVault",
      "keyVaultProperties": {
        "keyName": "KeyName",
        "keyVersion": "secretVersion",
        "keyVaultUri": "https://issue23056kv.vault.azure.net/"
      }
    }'

--encryption is CORRECT because it is used to specify encryption settings for the Azure Cognitive Services account, 
allowing you to configure customer-managed keys for data protection, which meets the requirement to use a customer-managed key.
 
***********************************************************************************************************
page no 261 below answer :




1. Upload a Glossary file to the french files container
2. Define a document translation specification that has french target
3. Perform asynchronous translation by using the document specification


225 pages continue

revice from 190 page 
Page 252 needs revise--> Docker containers


DALL-E images and api parameters 

Page no 190 to 194 in exam topic pdf 
Page number 249,253,255,261

277 --> need revision
287 
300 --? bot service
304 --> export model (language)
307 -->OnMembersAddedAsync
https://learn.microsoft.com/en-us/dotnet/api/microsoft.bot.builder.activityhandler.onmembersaddedasync?view=botbuilder-dotnet-stable
https://learn.microsoft.com/en-us/azure/bot-service/bot-builder-send-welcome-message?view=azure-bot-service-4.0&tabs=python

t

308 --<> Greet()
311 --> adoptavie card


${user.name} retrieves the user's name using a prompt.-->No.
In this context, ${user.name} directly retrieves the user's name from the incoming user object. It doesn't obtain the user's name through a prompt but assumes that the user object already contains the name property.

Greet() is the name of the language generation template.-->Yes.
# Greet(user) defines a language generation template named Greet. This template takes a user parameter, 
indicating that Greet is the template name used to generate personalized greeting messages.

${Greeting()} is a reference to a template in the language generation file.-->Yes.

309 ---> bot service 
315 ---> dialoue -->property state
317 ---> chatbot speech capabilites
318 --->cancel activity --> conversational activty flow
327 ---> bot steps 
328 --> bot activity handlers -->custom steps 
330  --> 5 bots into one  (install , create , recogniser/dispatch)_
342 --> aend trace activity
345 ---> bot logins crendetials 
349 --> docker run anamloy detetor service  --->very bad memory of you. Need understanding
354 --> chatgpt assitnat vs system difference answer is assistant
357 ---> tokens

QnA service pratical needed --->practise needed

questions from 402 needs to continue 




